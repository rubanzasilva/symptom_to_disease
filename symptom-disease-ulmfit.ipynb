{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5312008,"sourceType":"datasetVersion","datasetId":3087304},{"sourceId":11029643,"sourceType":"datasetVersion","datasetId":6869078},{"sourceId":11030552,"sourceType":"datasetVersion","datasetId":6869787}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: Predictive model for differential diagnosis\n\nIn this notebook, our goal is to develop a model that can take in a patient's symptoms as an input and return a list of the top 3 possible classes (diseases) alongside confidence values for each class expressed as probabilities.\n","metadata":{}},{"cell_type":"markdown","source":"## Library and Data import","metadata":{}},{"cell_type":"code","source":"#|include: false \n#|cellmetadata: false\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running the above (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#|include: false \n\n%pip install seaborn\n%pip install fastkaggle\n%pip install -Uqq fastbook\n%pip install --upgrade pip\n%pip install tqdm\n#%pip install catboost\n#%pip install optuna\n#%pip install optuna_distributed\n#%pip install openfe\n#%pip install xgboost\n#%pip install lightgbm\n#%pip install h2o\n#%pip install polars\n#%pip install -q -U autogluon.tabular\n#%pip install autogluon\n#%pip install wandb\n#%pip install sweetviz","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-17T11:56:34.553Z"},"_kg_hide-output":true,"scrolled":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| code-fold: true\n#| output: false\n#| code-summary: \"Library Import\"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import fastbook\n#fastbook.setup_book()\n#from fastbook import *\nfrom fastai.tabular.all import *\nimport numpy as np\nfrom numpy import random\nfrom tqdm import tqdm\nfrom ipywidgets import interact\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\nfrom fastai.text.all import *\nfrom pathlib import Path\nimport os\nimport warnings\nimport gc\nimport pickle\nfrom joblib import dump, load","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:09:50.817136Z","iopub.execute_input":"2025-03-16T13:09:50.817641Z","iopub.status.idle":"2025-03-16T13:09:55.633373Z","shell.execute_reply.started":"2025-03-16T13:09:50.817586Z","shell.execute_reply":"2025-03-16T13:09:55.632192Z"},"_kg_hide-output":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ULMFiT approach\n\nIn traditional text transfer learning, We use a pre-trained model called a language model. The model we are also going to use in this example was initially trained on Wikipedia on the task of guessing the next word. We then fine-tuned this model for our disease classification task based on symptoms. We can then use this model for our task of disease classification.\n\nBut the Wikipedia English might differ from medical jargon, so to further improve our model. We can employ a technique shown in the [ULMFIT Paper](https://arxiv.org/abs/1801.06146) by Jeremy Howard and Sebastian Ruder. They take the above a step further by fitting the pre-trained model on medical corpus and then using that as a base for our classifier. They noticed that adding this step of training the pretrained model on the task specific corpus gives better result as the model also has better context of the final task.","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/symptoms-disease-no-id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:09:55.634910Z","iopub.execute_input":"2025-03-16T13:09:55.635689Z","iopub.status.idle":"2025-03-16T13:09:56.750208Z","shell.execute_reply.started":"2025-03-16T13:09:55.635631Z","shell.execute_reply":"2025-03-16T13:09:56.748573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = Path('/kaggle/input/symptoms-disease-no-id')\npath","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:09:56.752531Z","iopub.execute_input":"2025-03-16T13:09:56.753054Z","iopub.status.idle":"2025-03-16T13:09:56.764280Z","shell.execute_reply.started":"2025-03-16T13:09:56.753002Z","shell.execute_reply":"2025-03-16T13:09:56.763282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#symptom_df = pd.read_csv(path_lm/'symptom_synth.csv',index_col=0)\nsymptom_df = pd.read_csv(path/'symptom_no_id.csv')\nsd_df = pd.read_csv(path/'symptom_disease_no_id_col.csv')\nsymptom_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:10:11.832655Z","iopub.execute_input":"2025-03-16T13:10:11.833063Z","iopub.status.idle":"2025-03-16T13:10:11.912557Z","shell.execute_reply.started":"2025-03-16T13:10:11.833024Z","shell.execute_reply":"2025-03-16T13:10:11.911222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"symptom_df['text'].nunique(),sd_df['text'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:10:12.253080Z","iopub.execute_input":"2025-03-16T13:10:12.253622Z","iopub.status.idle":"2025-03-16T13:10:12.268798Z","shell.execute_reply.started":"2025-03-16T13:10:12.253571Z","shell.execute_reply":"2025-03-16T13:10:12.267617Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finetuning a language model with my medical corpus","metadata":{}},{"cell_type":"markdown","source":"Below I define a DataLoader which is an extension of PyTorch's DataLoaders class, albeit with more functionality. This takes in our data, and prepares it as input for our model, passing it in batches etc.\n\nThe DataLoaders Object allows us to build data objects we can use for training without specifically changing the raw input data.\n\nThe dataloader then acts as input for our models. We also pass in valid_pct=0.2 which samples and uses 20% of our data for validation.","metadata":{}},{"cell_type":"code","source":"#dls_lm = TextDataLoaders.from_df(symptom_df, path=path, is_lm=True, valid_pct=0.2)\ndls_lm = TextDataLoaders.from_df(symptom_df, path=path, is_lm=True,text_col='text', valid_pct=0.2)\n#dls_lm = TextDataLoaders.from_folder(path=path_lm, is_lm=True, valid_pct=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:23:49.340923Z","iopub.execute_input":"2025-03-16T13:23:49.342061Z","iopub.status.idle":"2025-03-16T13:23:50.396779Z","shell.execute_reply.started":"2025-03-16T13:23:49.341961Z","shell.execute_reply":"2025-03-16T13:23:50.395543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We then use show_batch to have a look at some of our data.Since, we are guessing the next word in a sentence, you will notice that the targets have shifted one word to thr right in the *text_* column.","metadata":{}},{"cell_type":"code","source":"dls_lm.show_batch(max_n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:23:50.399186Z","iopub.execute_input":"2025-03-16T13:23:50.399567Z","iopub.status.idle":"2025-03-16T13:23:50.565866Z","shell.execute_reply.started":"2025-03-16T13:23:50.399531Z","shell.execute_reply":"2025-03-16T13:23:50.564559Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the above, we notice that the texts were processed and split into tokens. It adds some special tokens like xxbos to indicate the beginning of a text and xxmaj to indicate the next word was capitalised.\n\nWe then define a fastai [learner](https://docs.fast.ai/learner.html#learner), which is a fastai class that we can use to handle the training loop. It bundles the essential components needed for training together such as the data, model, the dataloaders, loss functions\n\nWe use the AWD LSTM architecture. We are also going to use accuracy and perplexity (the Exponential of the loss) as our metrics for this example. Furthermore, we also set a weight decay (wd) of 0.1 and apply mixed precision (.to_fp16()) to the learner, which speeds up training on GPU'S with tensor cores.\n","metadata":{}},{"cell_type":"code","source":"learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:23:50.567532Z","iopub.execute_input":"2025-03-16T13:23:50.568002Z","iopub.status.idle":"2025-03-16T13:23:51.277311Z","shell.execute_reply.started":"2025-03-16T13:23:50.567955Z","shell.execute_reply":"2025-03-16T13:23:51.276044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Phased Finetuning\n\nA pre-trained model is one that has already been trained on a large dataset and has learnt general patterns and features in a dataset, which can then be used to fine-tune to a specific task. \n\nBy default, the body of the model is frozen, meaning we won’t be updating the parameters of the body during training. For this case, only the head (first few layers) of the model will train.","metadata":{}},{"cell_type":"code","source":"#| error: false\nlearn.fit_one_cycle(1, 1e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:23:51.280305Z","iopub.execute_input":"2025-03-16T13:23:51.280808Z","iopub.status.idle":"2025-03-16T13:25:11.505466Z","shell.execute_reply.started":"2025-03-16T13:23:51.280758Z","shell.execute_reply":"2025-03-16T13:25:11.504169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As shown below, we can use the *learn.save* to save the state of our model to a file in learn.path/models/ named “filename.pth”. You can use learn.load('filename') to load the content of this file.","metadata":{}},{"cell_type":"code","source":"#| code-fold: show\n\n# Create a directory to save the model\nos.makedirs('/kaggle/working/models', exist_ok=True)\n\n# Set the model directory for the learner\nlearn.model_dir = '/kaggle/working/models'\n\n# Now save the model\nlearn.save('1epoch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:25:11.508168Z","iopub.execute_input":"2025-03-16T13:25:11.508655Z","iopub.status.idle":"2025-03-16T13:25:11.709591Z","shell.execute_reply.started":"2025-03-16T13:25:11.508608Z","shell.execute_reply":"2025-03-16T13:25:11.708340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| error: false\nlearn = learn.load('1epoch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:25:11.711120Z","iopub.execute_input":"2025-03-16T13:25:11.711712Z","iopub.status.idle":"2025-03-16T13:25:11.780219Z","shell.execute_reply.started":"2025-03-16T13:25:11.711661Z","shell.execute_reply":"2025-03-16T13:25:11.779332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After training the head of the model, we unfreeze the rest of the body and finetune it alongside the head, except for our final layer, which converts activations into probabilities of picking each token in our vocabulary.","metadata":{}},{"cell_type":"code","source":"#| error: false\nlearn.unfreeze()\nlearn.fit_one_cycle(5, 1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:25:11.781256Z","iopub.execute_input":"2025-03-16T13:25:11.781594Z","iopub.status.idle":"2025-03-16T13:31:58.528472Z","shell.execute_reply.started":"2025-03-16T13:25:11.781561Z","shell.execute_reply":"2025-03-16T13:31:58.527063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model not including the final layers is called an encoder. We use fastai's *save_encoder* to save it as shown below.","metadata":{}},{"cell_type":"code","source":"#| code-fold: true\n#| output: false\n#| code-summary: \"Save the model\"\n# Now save the model\nlearn.save_encoder('finetuned')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:31:58.530219Z","iopub.execute_input":"2025-03-16T13:31:58.530745Z","iopub.status.idle":"2025-03-16T13:31:58.731427Z","shell.execute_reply.started":"2025-03-16T13:31:58.530691Z","shell.execute_reply":"2025-03-16T13:31:58.730183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, that our model has been trained to guess or generate the next word in a sentence, we can use it to create or generate new user inputs that start with the below user input text.","metadata":{}},{"cell_type":"code","source":"#| output: false\n#| error: false\nTEXT = \"I have running nose, stomach and joint pains\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:31:58.744024Z","iopub.execute_input":"2025-03-16T13:31:58.745192Z","iopub.status.idle":"2025-03-16T13:32:08.994195Z","shell.execute_reply.started":"2025-03-16T13:31:58.745114Z","shell.execute_reply":"2025-03-16T13:32:08.992976Z"},"_kg_hide-output":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\".join(preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:08.996663Z","iopub.execute_input":"2025-03-16T13:32:08.997091Z","iopub.status.idle":"2025-03-16T13:32:09.003509Z","shell.execute_reply.started":"2025-03-16T13:32:08.997045Z","shell.execute_reply":"2025-03-16T13:32:09.002286Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training a text classifier\n\nWe now gather and pass in data to train our text classifier.","metadata":{}},{"cell_type":"code","source":"#symptom_df = pd.read_csv(path_lm/'symptom_synth.csv',index_col=0)\n#sd_df = pd.read_csv(path_lm/'symptom_disease_no_id_col.csv')\nsd_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:09.005074Z","iopub.execute_input":"2025-03-16T13:32:09.005547Z","iopub.status.idle":"2025-03-16T13:32:09.021502Z","shell.execute_reply.started":"2025-03-16T13:32:09.005499Z","shell.execute_reply":"2025-03-16T13:32:09.020173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for NaN values in the label column\nprint(sd_df['label'].isna().sum())\n\n# If there are NaNs, you can drop those rows\n#df = df.dropna(subset=['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:09.022721Z","iopub.execute_input":"2025-03-16T13:32:09.023068Z","iopub.status.idle":"2025-03-16T13:32:09.032589Z","shell.execute_reply.started":"2025-03-16T13:32:09.023036Z","shell.execute_reply":"2025-03-16T13:32:09.031584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| output: false\n#| error: false\n#dls_clas = TextDataLoaders.from_df(sd_df, path=path,valid='test', text_vocab=dls_lm.vocab)\ndls_clas = TextDataLoaders.from_df(sd_df, path=path,valid='test',text_col='text',label_col='label', text_vocab=dls_lm.vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:09.033874Z","iopub.execute_input":"2025-03-16T13:32:09.034252Z","iopub.status.idle":"2025-03-16T13:32:10.207207Z","shell.execute_reply.started":"2025-03-16T13:32:09.034212Z","shell.execute_reply":"2025-03-16T13:32:10.205862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Passing in *text_vocab=dls_lm.vocab* passes in our previously defined vocabulary to our classifier. \n\n> To quote the fastai documentation, we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won’t make any sense.\n\nWhen you train a language model, it learns to associate specific patterns of numbers (weights) with specific tokens (words or subwords) in your vocabulary. \n\nEach token is assigned a unique index in the vocabulary, and the model's internal representations (the weights in the embedding layers and beyond) are organised according to these indices.\n\nThink of it like a dictionary where each word has a specific page number. The model learns that information about \"good\" is on page 382, information about \"movie\" is on page 1593, and so on. These \"page numbers\" (indices) must remain consistent for the weights to make sense.\n\nIf you were to use a different vocabulary when creating your classifier:\n.The token \"good\" might now be on page 746 instead of 382\n.The weights the model learned during language model training were specifically tied to the old index (382)\n\nNow when the classifier sees \"good\" and looks up page 746, it finds weights that were meant for some completely different word\n\n>This mismatch would render the carefully fine-tuned language model weights essentially random from the perspective of the classifier.","metadata":{}},{"cell_type":"code","source":"#| error: false\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:10.209037Z","iopub.execute_input":"2025-03-16T13:32:10.209465Z","iopub.status.idle":"2025-03-16T13:32:10.637549Z","shell.execute_reply.started":"2025-03-16T13:32:10.209427Z","shell.execute_reply":"2025-03-16T13:32:10.636176Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We then define our text classifier as shown above. Before training it, we load in the previous encoder.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nlearn.path = Path('/kaggle/working')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:10.639003Z","iopub.execute_input":"2025-03-16T13:32:10.639416Z","iopub.status.idle":"2025-03-16T13:32:10.645392Z","shell.execute_reply.started":"2025-03-16T13:32:10.639380Z","shell.execute_reply":"2025-03-16T13:32:10.643630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| error: false\nlearn = learn.load_encoder('finetuned')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:10.647172Z","iopub.execute_input":"2025-03-16T13:32:10.647646Z","iopub.status.idle":"2025-03-16T13:32:10.695204Z","shell.execute_reply.started":"2025-03-16T13:32:10.647597Z","shell.execute_reply":"2025-03-16T13:32:10.694033Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Discriminative Learning Rates & Gradual Unfreezing\n\n**Discriminative learning** rates means using different learning rates for different layers of the model. \n\nFor example, earlier layers (closer to the input) might get smaller learning rates, while the later layers (closer to the output) get larger learning rates.\n\n**Gradual unfreezing** is a technique where layers of the model are unfrozen (made trainable) incrementally during fine-tuning. \nInstead of unfreezing all layers at once, you start by unfreezing only the topmost layers (closest to the output) and train them first.\n\nUnlike computer vision applications where we unfreeze the model at once, gradual unfreezing has been shown to improve performance for NLP models.\n\n\n","metadata":{}},{"cell_type":"code","source":"len(dls_lm.vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:10.696474Z","iopub.execute_input":"2025-03-16T13:32:10.696927Z","iopub.status.idle":"2025-03-16T13:32:10.704966Z","shell.execute_reply.started":"2025-03-16T13:32:10.696878Z","shell.execute_reply":"2025-03-16T13:32:10.703554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| error: false\nlearn.fit_one_cycle(1, 2e-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:10.706938Z","iopub.execute_input":"2025-03-16T13:32:10.707429Z","iopub.status.idle":"2025-03-16T13:32:47.080893Z","shell.execute_reply.started":"2025-03-16T13:32:10.707379Z","shell.execute_reply":"2025-03-16T13:32:47.079363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| error: false\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:32:47.082924Z","iopub.execute_input":"2025-03-16T13:32:47.083438Z","iopub.status.idle":"2025-03-16T13:33:31.730762Z","shell.execute_reply.started":"2025-03-16T13:32:47.083386Z","shell.execute_reply":"2025-03-16T13:33:31.729496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:33:31.732722Z","iopub.execute_input":"2025-03-16T13:33:31.733247Z","iopub.status.idle":"2025-03-16T13:36:30.315423Z","shell.execute_reply.started":"2025-03-16T13:33:31.733192Z","shell.execute_reply":"2025-03-16T13:36:30.314386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learn.predict(\"I am having a running stomach, fever, general body weakness and have been getting bitten by mosquitoes often\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T11:27:56.845783Z","iopub.execute_input":"2025-03-17T11:27:56.846465Z","iopub.status.idle":"2025-03-17T11:27:57.362849Z","shell.execute_reply.started":"2025-03-17T11:27:56.846416Z","shell.execute_reply":"2025-03-17T11:27:57.361270Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am having a running stomach, fever, general body weakness and have been getting bitten by mosquitoes often\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"],"ename":"NameError","evalue":"name 'learn' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"#| code-fold: true\n#| code-summary: \"Click to see full code in one cell\"\n#| error: false\npath = Path('/kaggle/input/symptoms-disease-no-id')\n#symptom_df = pd.read_csv(path_lm/'symptom_synth.csv',index_col=0)\nsymptom_df = pd.read_csv(path/'symptom_no_id.csv')\nsd_df = pd.read_csv(path/'symptom_disease_no_id_col.csv')\ndls_lm = TextDataLoaders.from_df(symptom_df, path=path,text_col='text', is_lm=True, valid_pct=0.2)\nlearn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()\nlearn.fit_one_cycle(1, 1e-2)\n# Create a directory to save the model\nos.makedirs('/kaggle/working/models', exist_ok=True)\n# Set the model directory for the learner\nlearn.model_dir = '/kaggle/working/models'\n# Now save the model\nlearn.save('1epoch')\nlearn = learn.load('1epoch')\nlearn.unfreeze()\nlearn.fit_one_cycle(5, 1e-3)\n# Now save the model\nlearn.save_encoder('finetuned')\n\n\n#finetuning the classifier\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\ndls_clas = TextDataLoaders.from_df(sd_df, path=path,text_col='text',label_col='label', text_vocab=dls_lm.vocab)\nfrom pathlib import Path\nlearn.path = Path('/kaggle/working')\nlearn = learn.load_encoder('finetuned')\nlearn.fit_one_cycle(1, 2e-2)\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\nlearn.predict(\"I am having a running stomach, fever, general body weakness and have been getting bitten by mosquitoes often\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:36:30.499939Z","iopub.execute_input":"2025-03-16T13:36:30.500320Z","execution_failed":"2025-03-16T13:35:56.743Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# References\n\n[Fastai Documentation - Text Transfer Learning](https://docs.fast.ai/tutorial.text.html#the-ulmfit-approach)\n\nThe dataset for this competition was gotten from [here](https://www.kaggle.com/datasets/niyarrbarman/symptom2disease)\n\n## Next Steps\n\nUsing clinical guidelines as a medical corpus source.\nImplementing a newer arhictecture.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}